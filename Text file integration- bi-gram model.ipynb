{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67fec35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/pawanbtw/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pawanbtw/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/pawanbtw/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.util import bigrams\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf0fc08",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "802acc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Lowercasing\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Remove punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [token.translate(table) for token in tokens]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Remove empty strings\n",
    "    tokens = [token for token in tokens if token.strip()]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def preprocess_text_file(file_path, encodings=('utf-16', 'latin1', 'windows-1252')):\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                text = file.read()\n",
    "            return preprocess_text(text)\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "    raise ValueError(\"Unable to decode the file using the specified encodings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8e91bc",
   "metadata": {},
   "source": [
    "# Text file Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28217474",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/pawanbtw/Downloads/mytext.txt\"\n",
    "preprocessed_text = preprocess_text_file(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263c3e78",
   "metadata": {},
   "source": [
    "# Calculating bigram frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dfdcd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Counter to store bigram frequencies\n",
    "bigram_freq = Counter()\n",
    "\n",
    "# Extract bigrams from the preprocessed text\n",
    "preprocessed_bigrams = list(bigrams(preprocessed_text))\n",
    "bigram_freq.update(preprocessed_bigrams)\n",
    "\n",
    "# Initialize a dictionary to store the sum of the frequencies for each unique word1\n",
    "word1_freq_sum = defaultdict(int)\n",
    "\n",
    "# Calculate the sum of the frequencies for each unique word1\n",
    "for word1, word2 in bigram_freq:\n",
    "    word1_freq_sum[word1] += bigram_freq[(word1, word2)]\n",
    "\n",
    "# Initialize a dictionary to store bigram probabilities\n",
    "bigram_prob = defaultdict(float)\n",
    "\n",
    "# Calculate bigram probabilities\n",
    "for bigram, freq in bigram_freq.items():\n",
    "    word1, word2 = bigram\n",
    "    # Calculate the probability of word2 given word1\n",
    "    bigram_prob[bigram] = freq / word1_freq_sum[word1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1913db",
   "metadata": {},
   "source": [
    "# checking the pre-processed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84ecee02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alice', 'beginning', 'get', 'tired', 'sitting', 'sister', 'bank', 'nothing', 'twice', 'peeped', 'book', 'sister', 'reading', 'picture', 'conversation', 'use', 'book', 'thought', 'alice', 'without', 'picture', 'conversation', 'considering', 'mind', 'well', 'could', 'hot', 'day', 'made', 'feel', 'sleepy', 'stupid', 'whether', 'pleasure', 'making', 'daisychain', 'would', 'worth', 'trouble', 'getting', 'picking', 'daisy', 'suddenly', 'white', 'rabbit', 'pink', 'eye', 'ran', 'close', 'nothing', 'remarkable', 'alice', 'think', 'much', 'way', 'hear', 'rabbit', 'say', 'oh', 'dear', 'oh', 'dear', 'shall', 'late', 'thought', 'afterwards', 'occurred', 'ought', 'wondered', 'time', 'seemed', 'quite', 'natural', 'rabbit', 'actually', 'took', 'watch', 'waistcoatpocket', 'looked', 'hurried', 'alice', 'started', 'foot', 'flashed', 'across', 'mind', 'never', 'seen', 'rabbit', 'either', 'waistcoatpocket', 'watch', 'take', 'burning', 'curiosity', 'ran', 'across', 'field', 'time', 'see', 'pop', 'large', 'rabbithole', 'hedge', 'another', 'moment', 'went', 'alice', 'never', 'considering', 'world', 'get', 'rabbithole', 'went', 'straight', 'like', 'tunnel', 'way', 'dipped', 'suddenly', 'suddenly', 'alice', 'moment', 'think', 'stopping', 'found', 'falling', 'deep', 'well', 'either', 'well', 'deep', 'fell', 'slowly', 'plenty', 'time', 'went', 'look', 'wonder', 'going', 'happen', 'next', 'first', 'tried', 'look', 'make', 'coming', 'dark', 'see', 'anything', 'looked', 'side', 'well', 'noticed', 'filled', 'cupboard', 'bookshelf', 'saw', 'map', 'picture', 'hung', 'upon', 'peg', 'took', 'jar', 'one', 'shelf', 'passed', 'labelled', 'orange', 'marmalade', 'great', 'disappointment', 'empty', 'like', 'drop', 'jar', 'fear', 'killing', 'somebody', 'managed', 'put', 'one', 'cupboard', 'fell', 'past', 'well', 'thought', 'alice', 'fall', 'shall', 'think', 'nothing', 'tumbling', 'stair', 'brave', 'think', 'home', 'would', 'nt', 'say', 'anything', 'even', 'fell', 'top', 'house', 'likely', 'true']\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5d77e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. bank: 0.5\n",
      "2. reading: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Example: Predicting the next word given two previous words\n",
    "previous_words = (\"look\", \"sister\")\n",
    "next_word_candidates = [(word2, prob) for (word1, word2), prob in bigram_prob.items() if word1 == previous_words[-1] and word2 not in previous_words]\n",
    "next_word_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the next word candidates line by line\n",
    "for i, (word, prob) in enumerate(next_word_candidates[:5], start=1):\n",
    "    print(f\"{i}. {word}: {prob}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62925a41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
