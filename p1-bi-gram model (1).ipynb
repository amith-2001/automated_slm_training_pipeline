{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb844e3d",
   "metadata": {},
   "source": [
    "# Download NLTK resources "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53d67f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/pawanbtw/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/pawanbtw/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pawanbtw/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/pawanbtw/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from nltk.util import bigrams\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c54ce02",
   "metadata": {},
   "source": [
    "# Preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b187daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Lowercasing\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Remove punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [token.translate(table) for token in tokens]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Remove empty strings\n",
    "    tokens = [token for token in tokens if token.strip()]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4956e41",
   "metadata": {},
   "source": [
    "# Preprocess Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5e02617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Brown Corpus\n",
    "brown_corpus = brown.sents()\n",
    "\n",
    "# Preprocess the data\n",
    "preprocessed_corpus = []\n",
    "for sentence in brown_corpus:\n",
    "    # Join the words into a single string\n",
    "    text = ' '.join(sentence)\n",
    "    # Apply preprocessing\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    # Add preprocessed text to corpus\n",
    "    preprocessed_corpus.append(preprocessed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb4e8c7",
   "metadata": {},
   "source": [
    "# Calculate Bigram Frequencies and Probabilities\n",
    "## Counter for bigram frequencies, precomputing sums for each unique word1, and implementing Good-Turing smoothing for probability calculation, improving efficiency and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac49dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Initialize a Counter to store bigram frequencies\n",
    "bigram_freq = Counter()\n",
    "\n",
    "# Iterate through each preprocessed sentence\n",
    "for sentence in preprocessed_corpus:\n",
    "    # Extract bigrams from the preprocessed sentence\n",
    "    sentence_bigrams = list(bigrams(sentence))\n",
    "    \n",
    "    # Update bigram frequencies\n",
    "    bigram_freq.update(sentence_bigrams)\n",
    "\n",
    "# Initialize a dictionary to store the sum of the frequencies for each unique word1\n",
    "word1_freq_sum = defaultdict(int)\n",
    "\n",
    "# Calculate the sum of the frequencies for each unique word1\n",
    "for word1, word2 in bigram_freq.keys():\n",
    "    word1_freq_sum[word1] += bigram_freq[(word1, word2)]\n",
    "\n",
    "# Initialize a dictionary to store bigram probabilities\n",
    "bigram_prob = defaultdict(float)\n",
    "\n",
    "# Calculate bigram probabilities\n",
    "for bigram, freq in bigram_freq.items():\n",
    "    word1, word2 = bigram\n",
    "    # Calculate the probability of word2 given word1\n",
    "    bigram_prob[bigram] = freq / word1_freq_sum[word1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc66280",
   "metadata": {},
   "source": [
    "# Example: Predicting the Next Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7dd6dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ask: 0.012594458438287154\n",
      "2. presid: 0.012594458438287154\n",
      "3. feder: 0.010075566750629723\n",
      "4. refer: 0.010075566750629723\n",
      "5. note: 0.010075566750629723\n"
     ]
    }
   ],
   "source": [
    "# Example: Predicting the next word given two previous words\n",
    "previous_words = (\"investig\", \"report\")\n",
    "next_word_candidates = [(word2, prob) for (word1, word2), prob in bigram_prob.items() if word1 == previous_words[-1] and word2 not in previous_words]\n",
    "next_word_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "# Print the next word candidates line by line\n",
    "for i, (word, prob) in enumerate(next_word_candidates[:5], start=1):\n",
    "    print(f\"{i}. {word}: {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae8055ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. report: 0.036036036036036036\n",
      "2. question: 0.02702702702702703\n",
      "3. use: 0.018018018018018018\n",
      "4. made: 0.018018018018018018\n",
      "5. result: 0.018018018018018018\n"
     ]
    }
   ],
   "source": [
    "# Example: Predicting the next word given a previous word\n",
    "previous_word = \"investig\"\n",
    "next_word_candidates = [(word2, prob) for (word1, word2), prob in bigram_prob.items() if word1 == previous_word]\n",
    "next_word_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "# Print the next word candidates line by line\n",
    "for i, (word, prob) in enumerate(next_word_candidates[:5], start=1):\n",
    "    print(f\"{i}. {word}: {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8098091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. one: 0.01084010840108401\n",
      "2. special: 0.01084010840108401\n",
      "3. sharp: 0.008130081300813009\n",
      "4. leav: 0.008130081300813009\n",
      "5. specif: 0.008130081300813009\n"
     ]
    }
   ],
   "source": [
    "# Example: Predicting the next word given a previous word\n",
    "previous_word = \"rather\"\n",
    "next_word_candidates = [(word2, prob) for (word1, word2), prob in bigram_prob.items() if word1 == previous_word]\n",
    "next_word_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "# Print the next word candidates line by line\n",
    "for i, (word, prob) in enumerate(next_word_candidates[:5], start=1):\n",
    "    print(f\"{i}. {word}: {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "546964d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. two: 0.013729977116704805\n",
      "2. index: 0.013729977116704805\n",
      "3. one: 0.011441647597254004\n",
      "4. address: 0.009153318077803204\n",
      "5. never: 0.009153318077803204\n"
     ]
    }
   ],
   "source": [
    "# Example: Predicting the next word given a previous word\n",
    "previous_word = \"name\"\n",
    "next_word_candidates = [(word2, prob) for (word1, word2), prob in bigram_prob.items() if word1 == previous_word]\n",
    "next_word_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "# Print the next word candidates line by line\n",
    "for i, (word, prob) in enumerate(next_word_candidates[:5], start=1):\n",
    "    print(f\"{i}. {word}: {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39d81ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Predicting the next word given a previous word\n",
    "previous_word = \"ultimately\"\n",
    "next_word_candidates = [(word2, prob) for (word1, word2), prob in bigram_prob.items() if word1 == previous_word]\n",
    "next_word_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "# Print the next word candidates line by line\n",
    "for i, (word, prob) in enumerate(next_word_candidates[:5], start=1):\n",
    "    print(f\"{i}. {word}: {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "584ca975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. whether: 0.05263157894736842\n",
      "2. much: 0.05263157894736842\n",
      "3. decis: 0.02631578947368421\n",
      "4. color: 0.02631578947368421\n",
      "5. russia: 0.02631578947368421\n"
     ]
    }
   ],
   "source": [
    "# Example: Predicting the next word given a previous word\n",
    "previous_word = \"regardless\"\n",
    "next_word_candidates = [(word2, prob) for (word1, word2), prob in bigram_prob.items() if word1 == previous_word]\n",
    "next_word_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "# Print the next word candidates line by line\n",
    "for i, (word, prob) in enumerate(next_word_candidates[:5], start=1):\n",
    "    print(f\"{i}. {word}: {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "829c2c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. mantl: 0.023255813953488372\n",
      "2. wife: 0.023255813953488372\n",
      "3. u: 0.023255813953488372\n",
      "4. spirit: 0.015503875968992248\n",
      "5. convent: 0.015503875968992248\n"
     ]
    }
   ],
   "source": [
    "# Example: Predicting the next word given a previous word\n",
    "previous_word = \"neither\"\n",
    "next_word_candidates = [(word2, prob) for (word1, word2), prob in bigram_prob.items() if word1 == previous_word]\n",
    "next_word_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "# Print the next word candidates line by line\n",
    "for i, (word, prob) in enumerate(next_word_candidates[:5], start=1):\n",
    "    print(f\"{i}. {word}: {prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfce5a8d",
   "metadata": {},
   "source": [
    "## Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c579f40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Define the filename for storing the pickled model\n",
    "filename = 'bigram_model.pkl'\n",
    "\n",
    "# Pickle the model\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(bigram_prob, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ebc846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Define the filename from which to load the pickled model\n",
    "filename = 'bigram_model.pkl'\n",
    "\n",
    "# Load the pickled model\n",
    "with open(filename, 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e5e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "import pickle\n",
    "\n",
    "# Load the pickled model\n",
    "with open('bigram_model.pkl', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "def predict_next_word():\n",
    "    input_text = input_entry.get(\"1.0\", \"end-1c\")\n",
    "    previous_word = input_text.split()[-1]  # Get the last word as the previous word\n",
    "    next_word_candidates = [(word2, prob) for (word1, word2), prob in loaded_model.items() if word1 == previous_word]\n",
    "    next_word_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "    output_text = \"\\n\".join([f\"{word}: {prob}\" for word, prob in next_word_candidates[:5]])\n",
    "    output_textbox.delete(1.0, tk.END)\n",
    "    output_textbox.insert(tk.END, output_text)\n",
    "\n",
    "# Create the GUI window\n",
    "window = tk.Tk()\n",
    "window.title(\"Next Word Prediction\")\n",
    "\n",
    "# Create input field\n",
    "input_label = tk.Label(window, text=\"Enter text:\")\n",
    "input_label.pack()\n",
    "input_entry = scrolledtext.ScrolledText(window, wrap=tk.WORD, width=40, height=5)\n",
    "input_entry.pack()\n",
    "\n",
    "# Create predict button\n",
    "predict_button = tk.Button(window, text=\"Predict\", command=predict_next_word)\n",
    "predict_button.pack()\n",
    "\n",
    "# Create output area\n",
    "output_label = tk.Label(window, text=\"Next word candidates:\")\n",
    "output_label.pack()\n",
    "output_textbox = scrolledtext.ScrolledText(window, wrap=tk.WORD, width=40, height=5)\n",
    "output_textbox.pack()\n",
    "\n",
    "# Run the GUI application\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97361bf4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'os' has no attribute 'list_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_all\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'os' has no attribute 'list_all'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c7810b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
